# Week 01: 회귀 <sup>```~03.28```</sup> 
회귀(regression)에 대해 알아보고, sklearn를 통한 머신러닝 코드 기초를 학습했다.
## k-최근접 회귀 (Ch 03-1)
### 기본 코드
1. `train_test_split`로 train, test 데이터를 쉽게 나눌 수 있다.
2. numpy `reshape(a,b)` 함수로 array 크기(차원)를 조정할 수 있다. 파라미터로 `-1`을 넣으면 해당 위치의 크기를 자동으로 조절한다.
> sklearn에서는 train 데이터로 2차원 배열을 요구한다.
### 알고리즘
1. target 데이터(x)와 가장 근접한 k개의 데이터(x)의 y값 평균을 예측값으로 사용한다.
2. ```KNeighborsRegressor()```로 해당 모델을 만들 수 있다. (기본 k=3)
### 모델 평가
1. 모델 변수의 `.fit(x, y)`을 사용해 모델을 피팅하고, `.score(x,y)`를 통해 모델을 평가할 수 있다.
2. `.predict(x)`로 단일 값에 대한 예측값 y를 확인할 수 있다.
3. 결정계수($R^2$)로 모델을 평가한다.  
$R^2 = 1 - \frac{(타깃-예측)^2}{(타깃-평균)^2}$ 수식을 보면, 예측 결과가 정확할수록 1이 된다.
4. `mean_absolute_error(target, predict)`으로 절댓값 오차를 계산할 수도 있다.
### 과적합
1. train에서 충분한 학습이 되지 않을 경우 underfitting(과소적합), 너무 많이 반영될 경우 overfitting(과대적합)이 일어난다.
2. 과소적합이 일어날 경우, train 데이터를 충분히 준비하거나 모델을 더 복잡하게 하여 학습을 많이 시키면 개선될 수 있다.
3. 예제에서는 k-최근접 회귀의 k값을 3->5로 바꾸어 모델을 더 복잡하게 만들어 개선하였다.

## 선형 회귀 (Ch 03-2)
### 알고리즘
1. x, y의 관계를 선형 함수로 나타내도록 하는 것이다.
2. `LinearRegression()`로 해당 모델을 만들 수 있다.
3. 모델 변수의 `.coef_`로 계수(coefficient), 즉 기울기를 알 수 있고, `.intercept_`로 y절편을 알 수 있다.

### 다항회귀
1. 다항식을 사용하여 모델의 특징을 더 잘 설명할 수도 있다.
2. $b_1x^2+b_2x+c$를 보면 2차 방정식(비선형)으로 보이지만, $x^2$를 다른 변수로 치환하여 다른 설명변수 x로 사용한다면 선형 다항식이 된다.
3. `new_x = numpy.column_stack(x**2, x)` 방식으로 배열을 만들고, 모델 변수 `.fit`에서 `new_x`를 x로 사용하면 된다.
